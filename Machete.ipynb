{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndresMontesDeOca/RegresionAvanzada/blob/main/Machete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eda0864a",
      "metadata": {
        "id": "eda0864a"
      },
      "source": [
        "# MACHETE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logbook"
      ],
      "metadata": {
        "id": "ZrfIpM4oo-bq"
      },
      "id": "ZrfIpM4oo-bq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Autor: Andres Montes de Oca\n",
        "\n",
        "# 31/05/23 -> Creacion de la Notebook\n",
        "# 31/05/23 -> Tests Normalidad, Homocedasticidad Residuos\n",
        "# 06/06/23 -> Test No-autoorrelacion de Residuos\n",
        "# 17/06/23 -> Transfomraciones Box Clox\n",
        "# 19/06/23 -> Migrated to Google Colab\n",
        "# 20/06/23 -> R Magic\n",
        "# 22/06/23 -> Deteccion Outliers e Influyentes\n",
        "# 27/06/23 -> Cuadrados Minimos Ponderados (WLS)\n",
        "# 29/06/23 -> Modelos Robustos\n",
        "# 30/06/23 -> RLM: Seleccion de Variables\n",
        "# 01/07/23 -> Multicolinealidad y Validacion Simple\n",
        "# 12/07/23 -> Regresion Polinomica\n",
        "# 14/07/23 -> Metodos Regularizacion\n",
        "# 15/07/23 -> Comparacion de Modelos\n",
        "# 16/07/23 -> ANOVA\n",
        "# 18/07/23 -> Regresion Logistic"
      ],
      "metadata": {
        "id": "HO1BXrWJo83M"
      },
      "id": "HO1BXrWJo83M",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf8c17a",
      "metadata": {
        "id": "bbf8c17a"
      },
      "outputs": [],
      "source": [
        "# Instalacion de Paquetes\n",
        "!pip install pingouin # No incluido en Google Colab\n",
        "# !pip install scipy\n",
        "# !pip install statsmodels\n",
        "\n",
        "# Version rpy2 que no tiene problemas de compatibilidad\n",
        "!pip install rpy2==3.5.1\n",
        "\n",
        "# Cargamos Librerias y Datos\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "from scipy import stats as st\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.tools.tools as smt\n",
        "import math\n",
        "\n",
        "# Ignorar Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Asthetics\n",
        "sns.set(style='ticks', context='notebook', palette='colorblind', font_scale=1, color_codes=True)\n",
        "\n",
        "# Recursion limit errors with R Magic\n",
        "import sys\n",
        "# sys.setrecursionlimit(50000)\n",
        "\n",
        "# Activamos R magic\n",
        "%load_ext rpy2.ipython"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf6ec746",
      "metadata": {
        "id": "bf6ec746"
      },
      "source": [
        "### Libreria rpy2 (Python <==> R)\n",
        "- Usamos R Magic, asi que no las necesitamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "436c2819",
      "metadata": {
        "id": "436c2819"
      },
      "outputs": [],
      "source": [
        "# # Import rpy2 for dataframe conversion\n",
        "# import rpy2.robjects as ro\n",
        "# from rpy2.robjects.packages import importr\n",
        "# from rpy2.robjects import pandas2ri\n",
        "# from rpy2.robjects.conversion import localconverter\n",
        "# from rpy2.robjects import globalenv\n",
        "\n",
        "# ###### Pandas DataFrames and Series conversion ########\n",
        "# # Cargamos un DataSet cualquira en Python, para que no de error\n",
        "# data_P = sns.load_dataset('iris')\n",
        "# Serie = data_P['petal_length']\n",
        "\n",
        "# # Convert the Python DataFrame to the R dataframe\n",
        "# %R -i data_P\n",
        "\n",
        "# with localconverter(ro.default_converter + pandas2ri.converter):\n",
        "#   data_R = ro.conversion.py2rpy(data_P)\n",
        "# # Create a variable name in R's Global Environment\n",
        "# globalenv['data_R'] = data_R\n",
        "\n",
        "# # Convert Python Series to R vectors\n",
        "# vec_float_R = ro.vectors.FloatVector(Serie)\n",
        "# # vec_int_R = ro.vectors.IntVector(Serie)\n",
        "# # vec_str_R = ro.vectors.StrVector(Serie)\n",
        "# globalenv['vec_float_R'] = vec_float_R\n",
        "\n",
        "# # Convert R datadrame/vector to Python DataFrame/Vector\n",
        "# %R -o data_R\n",
        "\n",
        "# with localconverter(ro.default_converter + pandas2ri.converter):\n",
        "#   data_P = ro.conversion.rpy2py(data_R)\n",
        "\n",
        "# # Importar R-Functions a Python como Objetos(sin uso)\n",
        "# shapiro_test = ro.r('shapiro.test')\n",
        "# result = shapiro_test(vec_float_R)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instalaciones Individuales en Bash"
      ],
      "metadata": {
        "id": "KfM50m-wCz4b"
      },
      "id": "KfM50m-wCz4b"
    },
    {
      "cell_type": "code",
      "source": [
        "# system(sudo apt install libgsl-dev) # -> Rapido"
      ],
      "metadata": {
        "id": "pZI0hXx4DbdA"
      },
      "execution_count": 5,
      "outputs": [],
      "id": "pZI0hXx4DbdA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instalacion de Librerias para R en Google Colab"
      ],
      "metadata": {
        "id": "RWsE_ORpdjnL"
      },
      "id": "RWsE_ORpdjnL"
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "\n",
        "# ## MVN Henze-Zirkler Test ### -> Lento\n",
        "# # system(sudo apt install libgsl-dev)\n",
        "# install.packages('MVN')\n",
        "\n",
        "# ## QQ Plot, Bonferroni, outlierTest, vif, leveneTest ###\n",
        "# install.packages('car') # 10 min\n",
        "\n",
        "# # Kruskal-Wallis Multiple Comparation ###\n",
        "# install.packages('pgirmess') # 12 Min\n",
        "\n",
        "# # Metodos de Regularizacion y Reg Logistica ### ->\n",
        "# install.packages('glmnet') # 5 mins\n",
        "\n",
        "# # Estimaciones Robustas ### ->\n",
        "# install.packages('quantreg') # LAD 3:30 Min\n",
        "# # install.packages('robustbase') # lmrob Rapido\n",
        "# install.packages('olsrr') # 6 mins\n",
        "\n",
        "#######################################\n",
        "# # Herramienta para ANOVA ###\n",
        "# install.packages('lsr') # Rapido\n",
        "# install.packages('gridExtra') # Rapido\n",
        "\n",
        "# # Wald Test ###\n",
        "# install.packages('aod') # Rapido\n",
        "\n",
        "# LRM Tests ###\n",
        "# install.packages('lmtest') # Rapido\n",
        "\n",
        "# # Box-Cox Transformation ### -> Rapido\n",
        "# install.packages('MASS') # Rapido, Huber, LTS\n",
        "\n",
        "# # Seleccion de Variables ### -> Rapido\n",
        "# install.packages('leaps') # -> regsubsets\n",
        "\n",
        "\n",
        "# Test Hosmer-Lemeshow Regresion Logistica ### ->\n",
        "# install.packages('ResourceSelection') # Rapido\n",
        "\n",
        "# # Metodos de vcd ### ->\n",
        "# install.packages('vcd') # Rapido\n",
        "\n",
        "# # ROC Curve ### ->\n",
        "# install.packages('pROC') # 1 min\n",
        "\n",
        "# # Metodos de PCA ### -> Rapido\n",
        "# install.packages('pls')"
      ],
      "metadata": {
        "id": "8ufTcCWTEAvQ"
      },
      "id": "8ufTcCWTEAvQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b00fc712",
      "metadata": {
        "id": "b00fc712"
      },
      "source": [
        "## Cargas de Datasets from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74cd8b72",
      "metadata": {
        "id": "74cd8b72"
      },
      "outputs": [],
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "################################# Datasets ###################################\n",
        "# Grasa de Cerdos\n",
        "id = '153lGVzdixcHT-keKg8qmvaoWdPHg6_tB'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('grasacerdos.xlsx')\n",
        "dataG = pd.read_excel('grasacerdos.xlsx', index_col='Obs') # Cargamos el Dataset\n",
        "dataG = dataG.replace(to_replace=',', value='.', regex=True) # Reemplazo , por .\n",
        "dataG = dataG.astype('float') # Transformo en float\n",
        "\n",
        "######################################################\n",
        "# Peso, Edad, Colesterol\n",
        "id = '17Dv1WcWlc9ojWa6bnSfD7TCRaVKCKqj0'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('peso_edad_colest.xlsx')\n",
        "dataP = pd.read_excel('peso_edad_colest.xlsx')\n",
        "%R -i dataP\n",
        "######################################################\n",
        "# Cars con Modelo Lineal\n",
        "id = '15pf-6P4Ek2rp6mYmfOybHRqAxqOoTp6i'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('cars.csv')\n",
        "dataC = pd.read_csv('cars.csv')\n",
        "%R -i dataC\n",
        "model_cars = smf.ols('dist ~ speed', data=dataC).fit()\n",
        "######################################################\n",
        "# University\n",
        "id = '16kLQNXhyweAi38xR2IeQCCagVcU725UZ'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('University.csv')\n",
        "dataU = pd.read_csv('University.csv')\n",
        "######################################################\n",
        "# Iris\n",
        "dataI = sns.load_dataset('iris')\n",
        "%R -i dataI\n",
        "######################################################\n",
        "# Gorriones\n",
        "id = '15G6jIOMiuWaTs-qy7eUXwhED4delBMl6'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('gorriones.xlsx')\n",
        "dataO = pd.read_excel('gorriones.xlsx')\n",
        "dataO.rename(columns=\n",
        "            {'largototal':'Largo', 'extension ':'Alas', 'cabeza':'Cabeza',\n",
        "             'humero':'Pata', 'esternon':'Cuerpo', 'sobrevida ':'Target'},\n",
        "            inplace=True)\n",
        "%R -i dataO\n",
        "######################################################\n",
        "# Madera\n",
        "id = '17F8XFNpCszGHFfs7CpXzZotfmWmXXhoi'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('madera.csv')\n",
        "dataM = pd.read_csv('madera.csv').drop(columns='Unnamed: 0')\n",
        "%R -i dataM\n",
        "\n",
        "######################################################\n",
        "# Duncan\n",
        "id = '17aCADG_APoFTsCdPvdI5T25gBAUE8U2T'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('Duncan.csv')\n",
        "dataD = pd.read_csv('Duncan.csv')\n",
        "%R -i dataD\n",
        "\n",
        "######################################################\n",
        "# Infants\n",
        "id = '17qgVDqEcWeevZh2dHlvPIQ4ueAcTzSjU'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('low_birth_weight_infants.xlsx')\n",
        "dataF = pd.read_excel('low_birth_weight_infants.xlsx')\n",
        "%R -i dataF\n",
        "\n",
        "######################################################\n",
        "# MT Cars\n",
        "id = '17vmBKWIJUt3uiry6WH5-Tyu3ab_mVdX8'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('mtcars.csv')\n",
        "dataA = pd.read_csv('mtcars.csv')\n",
        "dataA.drop(columns='model', inplace=True)\n",
        "%R -i dataA\n",
        "\n",
        "######################################################\n",
        "# Fingerprints\n",
        "id = '17zjDDk7VZ1fb_6A-UOaDGpzLUyNeG5AW'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('fingerprints.csv')\n",
        "dataFP = pd.read_csv('fingerprints.csv').drop(columns='Unnamed: 0')\n",
        "%R -i dataFP\n",
        "\n",
        "######################################################\n",
        "# Carseats\n",
        "id = '17d0yuvZLyRBM5INORFMzlJSBzkL-I8wI'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('Carseats.csv')\n",
        "dataS = pd.read_csv('Carseats.csv')\n",
        "%R -i dataS\n",
        "\n",
        "######################################################\n",
        "# Azucar\n",
        "id = '186-x7TJCzFP5HZtSJXCZ3gCGnxNibQmo'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('azucar.xlsx')\n",
        "dataAz = pd.read_excel('azucar.xlsx')\n",
        "%R -i dataAz\n",
        "\n",
        "######################################################\n",
        "# Pulso\n",
        "id = '18EWFSQF3TImhb3fhgDz5DbqPDZ8lLZhC'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('pulso.xlsx')\n",
        "dataPul = pd.read_excel('pulso.xlsx')\n",
        "%R -i dataPul\n",
        "\n",
        "######################################################\n",
        "# Diabetes\n",
        "id = '15VkWfRjTlktOE_ZEVLjIzsOlHMsgwYAJ'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('diabetes.xls')\n",
        "dataDiab = pd.read_excel('diabetes.xls')\n",
        "%R -i dataDiab\n",
        "\n",
        "######################################################\n",
        "# Salmon\n",
        "id = '18GIi-Y7bovO75Y6SVL9Jw7cYsK3vSNJD'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('salmon.xlsx')\n",
        "dataSalmon = pd.read_excel('salmon.xlsx')\n",
        "%R -i dataSalmon\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis de Variables"
      ],
      "metadata": {
        "id": "Bj2eFW6ZFMUD"
      },
      "id": "Bj2eFW6ZFMUD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf05b562",
      "metadata": {
        "id": "cf05b562"
      },
      "outputs": [],
      "source": [
        "# # Relacion Lineal Grafica\n",
        "# sns.scatterplot(data=data, x='Largo', y='Alas')\n",
        "# plt.show()\n",
        "\n",
        "# # Multicollonearity - Eeach independent variable should be independent from other independent variables\n",
        "# print(pg.corr(data['Largo'], data['Alas'], method='pearson')) # Entre dos varibales, H0 -> Variables Independientes\n",
        "# print(pg.corr(data['Largo'], data['Alas'], method='spearman')) # Cuando no se cumplen los supuestos de Pearson\n",
        "# # %R cor.test(data$Largo, data$Alas, method='pearson')\n",
        "# display(sns.heatmap(data.corr(), vmin=-1, vmax=1, cmap='RdYlGn', annot=True)) # Todos contra todos\n",
        "\n",
        "# # Analsis de Noramlidad Multi-Variariable (Henze-Zirkler) -> Python\n",
        "# print(pg.multivariate_normality(data.drop(columns='Largo'))) # Henze-Zirkler Test, H0 -> Multiv Normal Dist\n",
        "\n",
        "# # Analsis de Noramlidad Multi-Variariable (Henze-Zirkler) -> R\n",
        "# # library(MVN)\n",
        "# # mvn_result <- mvn(data, mvnTest = 'hz')\n",
        "# # print(mvn_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Models"
      ],
      "metadata": {
        "id": "aqpTI97ucr0F"
      },
      "id": "aqpTI97ucr0F"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Linear Regression\n",
        "# %%R\n",
        "\n",
        "# model <- lm('Largo ~ Alas + Cabeza + Pata + Cuerpo', data=data)\n",
        "# resid <- model$resid\n",
        "# fitted <- model$fitted.values\n",
        "\n",
        "# print(summary(model))\n",
        "# print(anova(model1, model2)) # H0: Model 2 no aporta nada nuevo\n",
        "\n",
        "# # Bandas de Prediccion\n",
        "# predichos <- predict(object=model, interval='prediction', level=0.95) # newdata = newdata\n",
        "# nuevos_datos <- data.frame(predichos, data)\n",
        "# print(head(predichos))\n",
        "# print(head(nuevos_datos))\n",
        "# print(confint(model))\n",
        "\n",
        "# # Datos a Predecir (Python, other example)\n",
        "# # to_predict = [25, 48]\n",
        "# # newdata = pd.Series(to_predict, name='edad').to_frame()"
      ],
      "metadata": {
        "id": "H4kJn7W3Fas6"
      },
      "id": "H4kJn7W3Fas6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Graficamos en Python\n",
        "# %R -o nuevos_datos\n",
        "\n",
        "# # Grafico\n",
        "# sns.regplot(data=nuevos_datos, x='Alas', y='Largo')\n",
        "# sns.lineplot(data=nuevos_datos, x='Alas', y='lwr', linestyle='--', color='r', ci=None)\n",
        "# sns.lineplot(data=nuevos_datos, x='Alas', y='upr', linestyle='--', color='r', ci=None)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "0eh_UlkkFhXL"
      },
      "id": "0eh_UlkkFhXL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # LRM en Python\n",
        "\n",
        "# # Generamos el Modelo\n",
        "# data = sm.add_constant(data)\n",
        "# model = smf.ols('Largo ~ Alas + Cabeza + Pata + Cuerpo', data=data).fit()\n",
        "# print(model.summary())"
      ],
      "metadata": {
        "id": "BRe7V4KrcsGT"
      },
      "id": "BRe7V4KrcsGT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wald Test (revisar)\n",
        "Mide si la Variable X es lo suficientemente significativa como para explicar la"
      ],
      "metadata": {
        "id": "Vn93BkqUKkOb"
      },
      "id": "Vn93BkqUKkOb"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# library(aod)\n",
        "\n",
        "# # Wald Test para indentificar variables significativas\n",
        "# wald.test(Sigma = vcov(model), b = coef(model), Terms = 3) # Cuerpo| H0: Coef = 0 (Var no significativa)\n"
      ],
      "metadata": {
        "id": "u2u1biCGKpGn"
      },
      "id": "u2u1biCGKpGn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis de Diagnostico"
      ],
      "metadata": {
        "id": "UF-d2Q40JuGM"
      },
      "id": "UF-d2Q40JuGM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residuals Nomrality"
      ],
      "metadata": {
        "id": "6PS87hllKYD-"
      },
      "id": "6PS87hllKYD-"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# # Analisis de Normalidad en los Residuos\n",
        "# library(car)\n",
        "# qqPlot(resid) # Observaciones que romperian con el supuesto de Normalidad\n",
        "# print(shapiro.test(resid))"
      ],
      "metadata": {
        "id": "eSLP7Yw4KWnH"
      },
      "id": "eSLP7Yw4KWnH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Python\n",
        "\n",
        "# # Tests Analiticos\n",
        "# display(pg.normality(data['Alas'])) # Shapiro Test(single Var), H0 -> Normal Dist\n",
        "# print(st.anderson(data['Alas'], dist='norm')) # Anderson-Darling SciPy\n",
        "# print(st.kstest(data['Alas'], 'norm'))# Kolmogorov-Smirnov SciPy\n",
        "\n",
        "# # Tests Graficos\n",
        "# pg.qqplot(data['Alas'])\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "GHx3kYbQH16s"
      },
      "id": "GHx3kYbQH16s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6a56b196",
      "metadata": {
        "id": "6a56b196"
      },
      "source": [
        "### Residuals Variance (Homosedasticity)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Analisis Analitico (Breusch-Pagan Test) -> R | H0: Residuos Homocedasticos\n",
        "# %%R\n",
        "\n",
        "# library(lmtest)\n",
        "# bptest(model)"
      ],
      "metadata": {
        "id": "aM8aPx10LXlm"
      },
      "id": "aM8aPx10LXlm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94129b70",
      "metadata": {
        "id": "94129b70"
      },
      "outputs": [],
      "source": [
        "# # # Analsis Grafico -> Python\n",
        "\n",
        "# # Importamos desde R\n",
        "# %R -o resid\n",
        "# %R -o fitted\n",
        "\n",
        "# plt.scatter(x=fitted, y=resid)\n",
        "# plt.xlabel( 'Prediccion')\n",
        "# plt.ylabel('Residuo')\n",
        "# plt.title('Distribucion de Residuos')\n",
        "# plt.axhline(color='grey', linestyle='dashed', alpha=0.5)\n",
        "# plt.show()\n",
        "# # No se observa estructura de embudo\n",
        "\n",
        "# # # Analisis Analitico Python\n",
        "# # BP, p_value, _, _ =sm.stats.het_breuschpagan(model.resid, model.model.exog)\n",
        "# # print('Estadistico BP y p-value:', BP, p_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5bc19b",
      "metadata": {
        "id": "cd5bc19b"
      },
      "source": [
        "### Residuals Auto-Correlation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Analisis Analitico (Durbin-Watson Test) -> R | H0 No Auto-Correlacion\n",
        "# # 2=No Correlacion (Independecia)| 0=Correlacion Pos | 4=Correlacion Neg\n",
        "# %%R\n",
        "\n",
        "# library(lmtest)\n",
        "# dwtest(model, alternative='two.sided', iterations=1000)"
      ],
      "metadata": {
        "id": "aLbQWQRJMp2S"
      },
      "id": "aLbQWQRJMp2S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3fbc04",
      "metadata": {
        "id": "fb3fbc04"
      },
      "outputs": [],
      "source": [
        "# # Analsis Grafico -> Python\n",
        "\n",
        "# # Importamos desde R\n",
        "# %R -o resid\n",
        "# %R -o fitted\n",
        "\n",
        "# plt.scatter(x=data.index, y=resid)\n",
        "# plt.xlabel( 'Index')\n",
        "# plt.ylabel('Residuo')\n",
        "# plt.title('Correlacion de Residuos')\n",
        "# plt.axhline(color='grey', linestyle='dashed', alpha=0.5)\n",
        "# plt.show()\n",
        "# # No se observa estructura\n",
        "\n",
        "# # Analisis Analitico -> Python\n",
        "# print('Durbin-Watson:', sm.stats.durbin_watson(resid)) # Sin validacion Estadistica"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformaciones\n",
        "- Cuando los residuos no siguen una distribucion Normal -> Transformamos la y (BoxCox)\n",
        "- Cuando los residuos tienen estructura -> Transformamos/Agregamos X"
      ],
      "metadata": {
        "id": "vevQU54sOiTx"
      },
      "id": "vevQU54sOiTx"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Box Cox Transformation -> R\n",
        "# %%R\n",
        "# data = dataC\n",
        "# library(MASS)\n",
        "\n",
        "# # Modelo existente\n",
        "# model<- lm('dist ~ speed', data=data)\n",
        "\n",
        "# # Buscamos el Lambda  optimo -> Si Lambda  = 0 -> log10(y) | y**Lambda\n",
        "# boxcox(object = model, plotit=TRUE)"
      ],
      "metadata": {
        "id": "yy2EvWj_OuWR"
      },
      "id": "yy2EvWj_OuWR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Generamos el nuevo modelo con las nuevas variables transformadas, y veridicamos su Normalidad\n",
        "# %%R\n",
        "\n",
        "# lmb = 0.5\n",
        "\n",
        "# model_2 <- lm('dist**lmb ~ speed', data=data)\n",
        "# print(shapiro.test(model_2$resid))\n",
        "\n",
        "# # Bandas de Prediccion\n",
        "# predichos <- predict(object=model_2, interval='prediction', level=0.95)\n",
        "# nuevos_datos <- data.frame(predichos, data, data$dist**lmb)\n",
        "# print(head(nuevos_datos))\n",
        "\n",
        "# # # Reverse Transformation\n",
        "# # base = 2\n",
        "# # exp = 3\n",
        "# # print(base**exp)\n",
        "# # print(log(base**exp, base))"
      ],
      "metadata": {
        "id": "ONnQf1q_SnFj"
      },
      "id": "ONnQf1q_SnFj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Graficamos con las Bandas de Confianza y Prediccion\n",
        "# %R -o nuevos_datos # Exportamos desde R\n",
        "# %R -o lmb\n",
        "\n",
        "# sns.regplot(data=nuevos_datos, x='speed', y='data.dist.lmb')\n",
        "# sns.lineplot(data=nuevos_datos, x='speed', y='lwr', linestyle='--', color='r')\n",
        "# sns.lineplot(data=nuevos_datos, x='speed', y='upr', linestyle='--', color='r')\n",
        "# plt.ylabel('Transformed Distance')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "VaQNrMgoLw9k"
      },
      "id": "VaQNrMgoLw9k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Boc Cox Transformation -> Python\n",
        "# data = dataC\n",
        "# y = data['dist']\n",
        "\n",
        "# # Calculo Lambda\n",
        "# maxlog = st.boxcox(y)[1]\n",
        "# st.boxcox_normplot(y, -2, 2, plt)\n",
        "# plt.axvline(maxlog, color='red')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "U_d9p-9Bd_N2"
      },
      "id": "U_d9p-9Bd_N2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deteccion de Valores Outliers e Influyentes"
      ],
      "metadata": {
        "id": "_oOsrltML6nr"
      },
      "id": "_oOsrltML6nr"
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# data <- dataU\n",
        "# # library(car)\n",
        "\n",
        "# # Generamos un modelo base\n",
        "# model <- lm('nassets ~ stfees', data=data)\n",
        "# resid = model$resid\n",
        "# predicted = predict(model)\n",
        "# fitted = model$fitted.values\n",
        "\n",
        "# # Outliers - Bonferroni\n",
        "# qqPlot(resid)\n",
        "# outlierTest(model)\n",
        "# influenceIndexPlot(model, vars='Bonf')\n",
        "\n",
        "# # Influyentes - Leverage\n",
        "# crit_leverage <- 3*mean(hatvalues(model))\n",
        "# leverage <- hatvalues(model) > crit_leverage\n",
        "# leverage_data <- data.frame(cbind(hatvalues(model), crit_leverage, leverage))\n",
        "# print(subset(leverage_data, leverage==1))\n",
        "# # hist(hatvalues(model))\n",
        "\n",
        "# # Influyentes - Cook\n",
        "# # print(cooks.distance(model))\n",
        "# influenceIndexPlot(model, vars='Cook')\n",
        "\n",
        "# # Influyentes - DFFITS\n",
        "# p <- length(model$coef)\n",
        "# n <- NROW(model$resid)\n",
        "# dffits_crit <- 2*sqrt(p/n)\n",
        "# dffits <- dffits(model)\n",
        "# data_dffits <- data.frame(dffits=dffits)\n",
        "\n",
        "# # Influyentes - DFBetas\n",
        "# dfbetas_crit <- 1 # Umbral estandard\n",
        "# dfbetas_data <- dfbetas(model)[,2]>dfbetas_crit\n",
        "# print(which(dfbetas_data))\n",
        "\n",
        "\n",
        "# # Resumen\n",
        "# plot(model)\n",
        "# summary(influence.measures(model=model))\n",
        "# influencePlot(model = model)"
      ],
      "metadata": {
        "id": "1AFS2DQ0e5ow"
      },
      "id": "1AFS2DQ0e5ow",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # DFFITS Grafico -> Python\n",
        "# %R -o data_dffits\n",
        "# %R -o dffits_crit\n",
        "\n",
        "# # Grafico\n",
        "# data_dffits.head()\n",
        "# sns.scatterplot(data=data_dffits, x=data_dffits.index, y='dffits')\n",
        "# plt.axhline(y=dffits_crit, linestyle='--', color='red')\n",
        "# plt.axhline(y=-dffits_crit, linestyle='--', color='red')\n",
        "# plt.xticks(rotation=45)\n",
        "# plt.show()\n",
        "\n",
        "# # Filtro los criticos\n",
        "# dffits_crit_low = -dffits_crit\n",
        "# dffits_crit_up = dffits_crit\n",
        "\n",
        "# display(data_dffits.query('dffits > @dffits_crit_up or dffits < @dffits_crit_low'))"
      ],
      "metadata": {
        "id": "aCwhB3lvRAll"
      },
      "id": "aCwhB3lvRAll",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cuadrados Minimos Ponderados (WLS)\n",
        "- Cuando los residuos son Heterocedasticos\n",
        "- Se compensa la diferencia en los residuos, asignandole diferentes pesos a las observaciones"
      ],
      "metadata": {
        "id": "cXZiAP2wWaEJ"
      },
      "id": "cXZiAP2wWaEJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cuadrados Minimos Ponderados (WLS) en R\n",
        "# %%R\n",
        "\n",
        "# library(lmtest)\n",
        "\n",
        "# # Generamos un modelo base\n",
        "# model_ols <- lm('nassets ~ stfees', data=data)\n",
        "\n",
        "# # Generamos la ponderacion de pesos y modelamos de nuevo\n",
        "# peso1 = 1 / lm(abs(model_ols$resid) ~ model_ols$fitted.values)$fitted.values**2 # Funciona mejor\n",
        "# peso2 = 1 / model_ols$fitted.values**2 # A los valores mas alejados se le reduce el peso\n",
        "\n",
        "# # Modelamos con lo weigths\n",
        "# model_wls1 <- lm('nassets ~ stfees', data=data, weights=peso1)\n",
        "# model_wls2 <- lm('nassets ~ stfees', data=data, weights=peso2)\n",
        "\n",
        "# # Comparamos los modelos\n",
        "# plot(data$stfees, data$nassets, xlab='stfees', ylab='nassets', main='OLS vs WLS')\n",
        "# abline(model_ols, col='black')\n",
        "# abline(model_wls1, col='blue')\n",
        "# abline(model_wls2, col='red')\n"
      ],
      "metadata": {
        "id": "VbT2Q9T9WaNj"
      },
      "id": "VbT2Q9T9WaNj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos Robustos\n",
        "- Integran las observaciones atipicas con una menor ponderacion\n",
        "- Cuando Box Cox transformation no  ayudan"
      ],
      "metadata": {
        "id": "2WkEzkAJEytv"
      },
      "id": "2WkEzkAJEytv"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# data <- dataD\n",
        "# library(MASS)\n",
        "# library(quantreg)\n",
        "\n",
        "\n",
        "# # Huber: Menos peso a residuos grandes. Cuadratica que empalma con lineal\n",
        "# duncan_Huber <- rlm(income ~ education, data=data, k2=1.345)\n",
        "# print(summary(duncan_Huber))\n",
        "\n",
        "# # Rousseeuw LTS: Desconsidera los residuos mas grandes, solo teniendo en cuenta los menores\n",
        "# duncan_LTS <- lqs(income ~ education, data=data, method='lts')\n",
        "# print(duncan_LTS)\n",
        "\n",
        "# # LAD: Suma de valores absolutos\n",
        "# duncan_LAD <- rq(income ~ education, data=data, tau=0.5)\n",
        "# print (duncan_LAD)\n",
        "\n",
        "# # lmrob (explocado por Cecilia), indica la signifcancia de las variables\n",
        "# library(robustbase)\n",
        "# model_rob <- lmrob(income ~ education, data=data, tau=0.5)\n",
        "# print(summary(model_rob))\n"
      ],
      "metadata": {
        "id": "sxyQxArYFJcE"
      },
      "id": "sxyQxArYFJcE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regresion Lineal Multiple"
      ],
      "metadata": {
        "id": "qF2YSZbzWNGp"
      },
      "id": "qF2YSZbzWNGp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seleccion de Variables"
      ],
      "metadata": {
        "id": "rvJSFPJmWWbu"
      },
      "id": "rvJSFPJmWWbu"
    },
    {
      "cell_type": "code",
      "source": [
        "# # BEST SUBSET\n",
        "# %%R\n",
        "# data <- dataS\n",
        "# library(leaps)\n",
        "\n",
        "# # Modelo Best Subset -> Consideramos todos los subconjuntos de variables predictoras posibles\n",
        "# model_todos <- regsubsets(Sales ~ ., data=data, nvmax=10) # Hasta 10 variables\n",
        "# model_todos_summary <- summary(model_todos)\n",
        "# # print(model_todos_summary)\n",
        "\n",
        "# # Generamos el DataSet y vemos que coeficientes elligio\n",
        "# data_model_todos <- data.frame(orden=1:10, adjr2=model_todos_summary$rsq, Cp=model_todos_summary$cp, Bic=model_todos_summary$bic)\n",
        "# print(data_model_todos)\n",
        "# print(which.max(model_todos_summary$adjr2))\n",
        "# print(which.min(model_todos_summary$cp))\n",
        "# print(which.min(model_todos_summary$bic))\n",
        "# # print(coef(model_todos, 7))"
      ],
      "metadata": {
        "id": "h4lTaoB__2YF"
      },
      "id": "h4lTaoB__2YF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Graficamos las Evaluaciones de Best Subset en Python\n",
        "# %R -o data_model_todos\n",
        "\n",
        "# sns.scatterplot(data=data_model_todos, x='orden', y='adjr2')\n",
        "# plt.show()\n",
        "# sns.scatterplot(data=data_model_todos, x='orden', y='Cp')\n",
        "# plt.show()\n",
        "# sns.scatterplot(data=data_model_todos, x='orden', y='Bic')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "dKf3ppPqQWMZ"
      },
      "id": "dKf3ppPqQWMZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# # Methodo Backward (igual que el Best Subset)\n",
        "# model_back <- regsubsets(Sales ~ ., data=data, nvmax=10, method='backward') # Hasta 10 variables\n",
        "# model_back_summary <- summary(model_back)\n",
        "# # print(model_todos_summary)\n",
        "\n",
        "# # Generamos el DataSet y vemos que coeficientes elligio\n",
        "# data_model_back <- data.frame(orden=1:10, adjr2=model_back_summary$rsq, Cp=model_back_summary$cp, Bic=model_back_summary$bic)\n",
        "# print(data_model_back)\n",
        "# # print(coef(model_back, 7))"
      ],
      "metadata": {
        "id": "30bf7dS5OXqc"
      },
      "id": "30bf7dS5OXqc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Seleccion de Variables Paso a Paso (Deb Version)\n",
        "# %%R\n",
        "\n",
        "# # Train-Test Split\n",
        "# set.seed(1123)\n",
        "# n = nrow(data)\n",
        "# index = sample(n, n*.8, replace=FALSE)\n",
        "# train_set = data[index,]\n",
        "# test_set = data[-index,]\n",
        "\n",
        "# # Creamos un Modelo para todas las Variables, y otro solo para el Intercepto\n",
        "# model_full <- lm(Sales ~ ., data=train_set)\n",
        "# model_int <- lm(Sales ~-., data=train_set)\n",
        "# scopeformula <- formula(model_full)\n",
        "\n",
        "# # Modelo Forward (arrancamos del modelo sin variables)\n",
        "# # model_fwd <- step(object=model_int, scope=scopeformula, direction='forward') # Imprime\n",
        "# # FwdSelection_AIC <- AIC(model_fwd)\n",
        "# print(summary(model_fwd))\n",
        "# print(FwdSelection_AIC)\n",
        "\n",
        "# Modelo Backward (arrancamos del modelo con todas las variables)\n",
        "# model_back <- step(object=model_full, scope=scopeformula, direction='backward')\n",
        "# BackSelection_AIC <- AIC(model_back)\n",
        "# print(summary(model_back))\n",
        "# print(BackSelection_AIC)\n",
        "\n",
        "# # # Stepwise\n",
        "# # model_stepwise <- step(object=model_full, scope=scopeformula, direction='both')\n",
        "# # StepwiseSelecion_AIC <- AIC(model_stepwise)\n",
        "# # # print(summary(model_stepwise))\n",
        "# # # print(StepwiseSelecion_AIC)\n",
        "\n",
        "# # # Generamos el Dataset de Salida\n",
        "# # data_AIC <- data.frame(model_fwd=FwdSelection_AIC, model_back=BackSelection_AIC, model_stepwise=StepwiseSelecion_AIC)\n",
        "# # rownames(data_AIC) = c('AIC')\n",
        "# # data_AIC"
      ],
      "metadata": {
        "id": "_wHtR-EtAQw7"
      },
      "id": "_wHtR-EtAQw7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multicolinealidad\n",
        "VIF > 5 son redundantes"
      ],
      "metadata": {
        "id": "8wF9XV4e9L1e"
      },
      "id": "8wF9XV4e9L1e"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Correlation Plot Python\n",
        "# data = dataA\n",
        "\n",
        "# display(sns.heatmap(data.corr(), vmin=-1, vmax=1, cmap='RdYlGn', annot=True)) # Todos contra todos\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "SbGdU9Kp9kN-"
      },
      "id": "SbGdU9Kp9kN-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # VIF\n",
        "# %%R\n",
        "\n",
        "# library(car)\n",
        "\n",
        "# model1 <- lm(mpg ~ drat + disp + hp + cyl, data=data)\n",
        "# # print(summary(model1))\n",
        "# vif1 <- vif(model1)\n",
        "# print(vif1)\n",
        "\n",
        "# model2 <- lm(mpg ~ drat + disp + hp, data=data)\n",
        "# # print(summary(model2))\n",
        "# vif2 <- vif(model2)\n",
        "# print(vif2)"
      ],
      "metadata": {
        "id": "0R4q8C429wMY"
      },
      "id": "0R4q8C429wMY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validacion Simple\n",
        "Separacion en Train y Test"
      ],
      "metadata": {
        "id": "VQ5tOSlpGy03"
      },
      "id": "VQ5tOSlpGy03"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Python\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# data = dataF\n",
        "# X = data.drop(columns='headcirc')\n",
        "# y = data['headcirc']\n",
        "\n",
        "# Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# train = pd.concat([Xtrain, ytrain], axis=1)\n",
        "# test = pd.concat([Xtest, ytest], axis=1)\n",
        "\n",
        "# %R -i train\n",
        "# %R -i test\n",
        "# %R -i Xtest\n",
        "# %R -i ytest"
      ],
      "metadata": {
        "id": "G1-l5lFIVeWQ"
      },
      "id": "G1-l5lFIVeWQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # R\n",
        "# %%R\n",
        "\n",
        "# Xtrain <- model.matrix(headcirc ~ ., data = train)[, -1]\n",
        "# ytrain <- train$headcirc\n",
        "# Xtest <- model.matrix(headcirc ~ ., data = test)[, -1]\n",
        "# ytest <- test$headcirc"
      ],
      "metadata": {
        "id": "Qf_Sh7lokXNC"
      },
      "id": "Qf_Sh7lokXNC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # R\n",
        "# %%R\n",
        "# data = dataF\n",
        "# set.seed(1)\n",
        "\n",
        "# # install.packages('leaps')\n",
        "# library(leaps)\n",
        "\n",
        "# # Train Test Split\n",
        "# train <- sample(x=1:100, size=67, replace=FALSE)\n",
        "\n",
        "# # Modelamos, entrenando con Train\n",
        "# model_mejores <- regsubsets(headcirc ~ ., data=data[train,], nvmax=5, method='forward')\n",
        "\n",
        "# # Vector de errores de los modelos\n",
        "# validation_error <- rep(NA, 5)\n",
        "\n",
        "# # Matriz de predicciones, con Test\n",
        "# test_matrix <- model.matrix(headcirc ~ ., data=data[-train,])\n",
        "\n",
        "# # Calculo y muestro el de menor error\n",
        "# for (i in 1:5){\n",
        "#     coeficientes <-coef(object=model_mejores, id=i)\n",
        "#     predictores <- test_matrix[, names(coeficientes)]\n",
        "#     predicciones <- predictores %*% coeficientes\n",
        "#     validation_error[i] <- mean((data$headcirc[-train] - predicciones)**2)\n",
        "# }\n",
        "# print(which.min(validation_error))\n"
      ],
      "metadata": {
        "id": "UWNwdLiGG2Ur"
      },
      "id": "UWNwdLiGG2Ur",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regresion Polinomica"
      ],
      "metadata": {
        "id": "N4euQJP_Oe5X"
      },
      "id": "N4euQJP_Oe5X"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# model_poly2 <- lm(formula = headcirc ~ poly(gestage,2)+poly(birthwt,2), data = data)\n",
        "# summary(model_poly2)"
      ],
      "metadata": {
        "id": "gHE6C3VRPxr1"
      },
      "id": "gHE6C3VRPxr1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metodos de Regularizacion"
      ],
      "metadata": {
        "id": "8N92eznPSdCY"
      },
      "id": "8N92eznPSdCY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ridge Regresion"
      ],
      "metadata": {
        "id": "UxJccDtte4Ed"
      },
      "id": "UxJccDtte4Ed"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "# library(glmnet)\n",
        "# data = dataF\n",
        "\n",
        "# X <- model.matrix(headcirc ~ ., data = data)[, -1]\n",
        "# y <- data$headcirc\n",
        "\n",
        "# modelos_ridge <- glmnet(x = X, y = y, alpha = 0)\n",
        "\n",
        "# #\n",
        "# plot(modelos_ridge, xvar = \"lambda\", label = TRUE)\n",
        "\n",
        "# #\n",
        "# cv_error_ridge <- cv.glmnet(x = X, y = y, alpha = 0, nfolds = 10, type.measure = \"mse\")\n",
        "# plot(cv_error_ridge)\n",
        "\n",
        "# #\n",
        "# print(cv_error_ridge$lambda.min)\n",
        "# print(cv_error_ridge$lambda.1se)\n",
        "\n",
        "# #\n",
        "# modelo_final_ridge <- glmnet(x = X, y = y, alpha = 0, lambda = cv_error_ridge$lambda.min)\n",
        "# print(coef(modelo_final_ridge))\n"
      ],
      "metadata": {
        "id": "FNO84jiiYXuF"
      },
      "id": "FNO84jiiYXuF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lasso Regresion"
      ],
      "metadata": {
        "id": "Q2taiA_0hkup"
      },
      "id": "Q2taiA_0hkup"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "# library(glmnet)\n",
        "# data = dataF\n",
        "\n",
        "# X <- model.matrix(headcirc ~ ., data = data)[, -1]\n",
        "# y <- data$headcirc\n",
        "\n",
        "# modelos_lasso <- glmnet(x = X, y = y, alpha = 1)\n",
        "\n",
        "# #\n",
        "# plot(modelos_lasso, xvar = \"lambda\", label = TRUE)\n",
        "\n",
        "# #\n",
        "# cv_error_lasso <- cv.glmnet(x = X, y = y, alpha = 1, nfolds = 10, type.measure = \"mse\")\n",
        "# plot(cv_error_lasso)\n",
        "\n",
        "# #\n",
        "# print(cv_error_lasso$lambda.min)\n",
        "# print(cv_error_lasso$lambda.1se)\n",
        "\n",
        "# #\n",
        "# modelo_final_lasso <- glmnet(x = X, y = y, alpha = 1, lambda = cv_error_lasso$lambda.min)\n",
        "\n",
        "# print(coef(modelo_final_lasso))\n"
      ],
      "metadata": {
        "id": "ZYdxQcRbhFeL"
      },
      "id": "ZYdxQcRbhFeL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelos Basados en PCA\n",
        "Solo variables Numericas"
      ],
      "metadata": {
        "id": "U7f_Lg68hsPM"
      },
      "id": "U7f_Lg68hsPM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PCR\n",
        "No es un metodo de seleccionde variables, ya que todas estan incluidas en cada componente"
      ],
      "metadata": {
        "id": "VSb-CGMVu0L0"
      },
      "id": "VSb-CGMVu0L0"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "# data = dataF\n",
        "\n",
        "# library(pls)\n",
        "\n",
        "# modelo_pcr <- pcr(headcirc ~ ., data = data, scale = TRUE, validation = \"CV\")\n",
        "# print(summary(modelo_pcr))\n",
        "\n",
        "# validationplot(modelo_pcr, val.type = \"RMSEP\")\n",
        "# print(which.min(x = modelo_pcr$validation$PRESS))"
      ],
      "metadata": {
        "id": "WnX5fzSjhuy3"
      },
      "id": "WnX5fzSjhuy3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PLS\n",
        "Ademas de buscar la mayor cantidad de varianza, predicen lo mejor posible (PCR Supervisado)"
      ],
      "metadata": {
        "id": "Fq2kWnKdu7As"
      },
      "id": "Fq2kWnKdu7As"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "# data = dataF\n",
        "\n",
        "# library(pls)\n",
        "\n",
        "# modelo_pls <- plsr(formula = headcirc ~ ., data = data, scale. = TRUE, validation = \"CV\")\n",
        "# print(summary(modelo_pls))\n",
        "\n",
        "# validationplot(modelo_pls, val.type = \"RMSEP\")\n",
        "# print(which.min(x = modelo_pls$validation$PRESS))"
      ],
      "metadata": {
        "id": "OmjCEcl6u8hx"
      },
      "id": "OmjCEcl6u8hx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparacion de Modelos"
      ],
      "metadata": {
        "id": "lnkbCf6HJ3I2"
      },
      "id": "lnkbCf6HJ3I2"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "# data = dataF\n",
        "\n",
        "# # 2/3 de las observaciones\n",
        "# set.seed(1)\n",
        "# indices_entrenamiento <- sample(x = 1:nrow(data), size = round(nrow(data) * (2/3)))\n",
        "# indices_test <- (1:nrow(data))[-indices_entrenamiento]\n",
        "\n",
        "# train <- data[indices_entrenamiento, ]\n",
        "# test <- data[indices_test, ]\n",
        "\n",
        "# # Separacion Train y Test\n",
        "# Xtrain <- model.matrix(headcirc ~ ., data = train)[, -1]\n",
        "# ytrain <- train$headcirc\n",
        "# Xtest <- model.matrix(headcirc ~ ., data = test)[, -1]\n",
        "# ytest <- test$headcirc"
      ],
      "metadata": {
        "id": "hNxwFsUklLky"
      },
      "id": "hNxwFsUklLky",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OLS"
      ],
      "metadata": {
        "id": "K40CU15XmBFo"
      },
      "id": "K40CU15XmBFo"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# #Ordinary least square (regresión por mínimos cuadrados)\n",
        "# modelo_OLS <- lm(formula = headcirc ~ ., data = train)\n",
        "# test_MSE_OLS <- mean((predict(modelo_OLS, test) - test$headcirc)^2)\n",
        "# test_MSE_OLS"
      ],
      "metadata": {
        "id": "LMuYYIpFJ5Vc"
      },
      "id": "LMuYYIpFJ5Vc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ridge"
      ],
      "metadata": {
        "id": "4PqWww4Ojf63"
      },
      "id": "4PqWww4Ojf63"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# # Modelamos\n",
        "# library(glmnet)\n",
        "# set.seed(1)\n",
        "\n",
        "# cv_error_ridge <- cv.glmnet(x=Xtrain, y = ytrain, alpha = 0, nfolds = 10, type.measure = \"mse\")\n",
        "# # Para obtener un ajuste mediante *ridge regression* se indica argumento alpha=0\n",
        "# modelo_ridge <- glmnet(x = Xtrain, y = ytrain, alpha = 0, lambda = cv_error_ridge$lambda.1se)\n",
        "# # Se almacenan las predicciones en una variable separada para no concatenar mucho codigo\n",
        "# predicciones <- predict(object = modelo_ridge, newx = Xtest, s = cv_error_ridge$lambda.1se, exact = TRUE)\n",
        "# test_MSE_ridge <- mean((predicciones - ytest)^2)\n",
        "# test_MSE_ridge"
      ],
      "metadata": {
        "id": "zwiKeMt2jicf"
      },
      "id": "zwiKeMt2jicf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lasso"
      ],
      "metadata": {
        "id": "Aff5v3HNllxE"
      },
      "id": "Aff5v3HNllxE"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# library(glmnet)\n",
        "# set.seed(1)\n",
        "# # Se identifica mediante k-cross-validation el mejor valor de lambda para lasso\n",
        "# cv_error_lasso <- cv.glmnet(x = Xtrain, y = ytrain, alpha = 1, nfolds = 10, type.measure = \"mse\")\n",
        "# # Para obtener un ajuste mediante lasso se indica argumento alpha=1\n",
        "# modelo_lasso <- glmnet(x = Xtrain, y = ytrain, alpha = 1, lambda = cv_error_lasso$lambda.1se)\n",
        "# # Se almacenan las predicciones en una variable separada\n",
        "# predicciones <- predict(object = modelo_lasso, newx = Xtest, s = cv_error_lasso$lambda.1se, exact = TRUE)\n",
        "# test_MSE_lasso <- mean((predicciones - ytest)^2)\n",
        "# test_MSE_lasso"
      ],
      "metadata": {
        "id": "-JMUrRZ5lneK"
      },
      "id": "-JMUrRZ5lneK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PCR"
      ],
      "metadata": {
        "id": "r6ujU9n5mEQm"
      },
      "id": "r6ujU9n5mEQm"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# library(pls)\n",
        "# set.seed(123)\n",
        "# modelo_pcr <- pcr(headcirc ~ ., data = train, scale = TRUE, validation = \"CV\")\n",
        "\n",
        "# # Ploteamos Numero de componentes optimas\n",
        "# validationplot(modelo_pcr, val.type = \"RMSEP\")\n",
        "\n",
        "# # Calculamos el error\n",
        "# predicciones <- predict(object = modelo_pcr, newdata = test, ncomp = 1)\n",
        "# test_MSE_PCR <- mean((predicciones - ytest)^2)\n",
        "# print(test_MSE_PCR)"
      ],
      "metadata": {
        "id": "hMgaIiGhmLbh"
      },
      "id": "hMgaIiGhmLbh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PLS"
      ],
      "metadata": {
        "id": "QJedydzEnAYY"
      },
      "id": "QJedydzEnAYY"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# set.seed(123)\n",
        "# modelo_pls <- plsr(headcirc ~ ., data = train, scale = TRUE, validation = \"CV\")\n",
        "\n",
        "# # Ploteamos Numero de componentes optimas\n",
        "# validationplot(modelo_pls, val.type = \"RMSEP\")\n",
        "\n",
        "# # Calculamos el Error\n",
        "# predicciones <- predict(object = modelo_pls, newdata = test, ncomp = 1)\n",
        "# test_MSE_PLS <- mean((predicciones - ytest)^2)\n",
        "# test_MSE_PLS"
      ],
      "metadata": {
        "id": "xVRFkg7EnB67"
      },
      "id": "xVRFkg7EnB67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resumen Comparacion de Modelos\n"
      ],
      "metadata": {
        "id": "eANbekOKnfd2"
      },
      "id": "eANbekOKnfd2"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# require(ggplot2)\n",
        "\n",
        "# metodo <- c(\"OLS\", \"Ridge\", \"LASSO\", \"PCR\", \"PLS\")\n",
        "# test_MSE <- c(test_MSE_OLS, test_MSE_ridge, test_MSE_lasso, test_MSE_PCR,test_MSE_PLS)\n",
        "# resultados <- data.frame(metodo, test_MSE)\n",
        "# print(resultados)\n",
        "\n",
        "# # Ploteamos\n",
        "# ggplot(data = resultados, aes(x = reorder(metodo, test_MSE), y = test_MSE)) +\n",
        "#   geom_bar(stat = \"identity\") +\n",
        "#   labs(x = \"Método de regresión\", y = expression(\"test MSE\")) + theme_bw()"
      ],
      "metadata": {
        "id": "1Ar3gGX0niKW"
      },
      "id": "1Ar3gGX0niKW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Regresoras Categoricas"
      ],
      "metadata": {
        "id": "TNJ7VKWTfn6t"
      },
      "id": "TNJ7VKWTfn6t"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Con interaccion"
      ],
      "metadata": {
        "id": "81JI_SICrxgK"
      },
      "id": "81JI_SICrxgK"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# # install.packages('carData')\n",
        "# # library(carData)\n",
        "\n",
        "# data = Salaries\n",
        "# # print(head(data))\n",
        "\n",
        "# # Sin interaccion\n",
        "# modelo_sin <- lm(salary ~ rank, data=data)\n",
        "# # print(summary(modelo_sin))\n",
        "\n",
        "# # Con interaccion\n",
        "# modelo_con <- lm(salary ~ yrs.service*rank, data=data)\n",
        "# # print(summary(modelo_con))\n",
        "\n",
        "# # Comparacion demodelos\n",
        "# print(anova(modelo_sin, modelo_con)) # H0: Modelo 2 no aporta mas insights\n",
        "\n",
        "# # OneHot ENcoder (cuando la cardinlaidad de la variable es mayor de 2)\n",
        "# rankFactor <- factor(data$rank)\n",
        "# # print(contrasts(rankFactor))"
      ],
      "metadata": {
        "id": "zrHG95_vfrqZ"
      },
      "id": "zrHG95_vfrqZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analisis de la Varianza (ANOVA)\n",
        "Estudia una Variable Continua, para distintos niveles de una Categorica"
      ],
      "metadata": {
        "id": "8RGoJ8ITtdRk"
      },
      "id": "8RGoJ8ITtdRk"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# # install.packages('car')\n",
        "# library(car)\n",
        "\n",
        "# # Generamos Dataset de Ejemplo\n",
        "# porcentaje<-c(rep(15,5),rep(20,5),rep(25,5),rep(30,5),rep(35,5))\n",
        "# resistencia<-c(7,7,15,11,9,12,17,12,18,18,14,18,18,19,19,19,25,22,19,23,7,10,11,15,11)\n",
        "# porcAlgodon <-data.frame(porcentaje,resistencia)\n",
        "\n",
        "# # Transformamos la Categorica a Factor, y Ploteamos\n",
        "# porcentaje.f=factor(porcentaje)\n",
        "# boxplot(resistencia~porcentaje.f)\n",
        "# # print(head(porcAlgodon))\n",
        "\n",
        "# # Modelamos con ANOVA -> H0: Medias de cada grupo = (Test F)\n",
        "# model_anova<- aov(resistencia~porcentaje.f)\n",
        "# summary(model_anova)"
      ],
      "metadata": {
        "id": "932pbLSOz4x5"
      },
      "id": "932pbLSOz4x5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validez de la salida de ANOVA"
      ],
      "metadata": {
        "id": "JK3s1ig-1UDE"
      },
      "id": "JK3s1ig-1UDE"
    },
    {
      "cell_type": "code",
      "source": [
        "# # ANOVA valido solo si se satisfacen los supuestos\n",
        "# %%R\n",
        "\n",
        "# # Test de Levene (Homocedasticidad, robusto) -> H0: Varianzas  de cada grupo =\n",
        "# print(leveneTest(resistencia~porcentaje.f))\n",
        "\n",
        "# # Test Shapiro: H0 Normalidad Distribucion de los residuos\n",
        "# print(shapiro.test(residuals(model_anova)))\n",
        "# qqPlot(model_anova$resid)\n",
        "\n",
        "# # Conclusion del test F de ANOVA"
      ],
      "metadata": {
        "id": "bu8i1bNn1YIn"
      },
      "id": "bu8i1bNn1YIn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparaciones a Posteriori\n",
        "Si se rechaza el test F de ANOVA. Que subgrupos son distintos?"
      ],
      "metadata": {
        "id": "7_MxG0cSQN1M"
      },
      "id": "7_MxG0cSQN1M"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tukey Test mutiple comparasions of means -> H0: Means are = for each group\n",
        "# %%R\n",
        "\n",
        "# print(TukeyHSD(model_anova,conf.level=0.95))"
      ],
      "metadata": {
        "id": "ABfuv5TaQUTl"
      },
      "id": "ABfuv5TaQUTl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANOVA no-parametrico (Ordinales)\n",
        "No requiere supuestos Normalidad ni Homocedasticidad"
      ],
      "metadata": {
        "id": "vQxmHao-_sSS"
      },
      "id": "vQxmHao-_sSS"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# # Generamos el Dataset\n",
        "# datos <- data.frame(condicion = c(rep(\"condicion1\", 18), rep(\"condicion2\", 18), rep(\"condicion3\", 18)), n_huevos = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 16, 27, 28, 29, 30, 51, 52, 53, 342, 40, 41, 42, 43, 44, 45, 46, 47, 48, 67, 88, 89, 90, 91, 92, 93, 94, 293, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 25, 36, 37, 58, 59, 60, 71, 72))\n",
        "# # head(datos)\n",
        "\n",
        "# # Ploteamos para verificar las formas de las distribuciones de cada subgrupo\n",
        "# plot1 <- ggplot(data = datos, mapping = aes(x = n_huevos, colour = condicion)) +\n",
        "#   geom_histogram() + theme_bw() + facet_grid(. ~ condicion) +\n",
        "#   theme(legend.position = \"none\")# + stat_bin(binwidth=30)\n",
        "\n",
        "# # Y las medias de cada uno tambien\n",
        "# plot2 <- ggplot(data = datos, mapping = aes(x = condicion, y = n_huevos, colour = condicion)) +\n",
        "#   geom_boxplot() + theme_bw() + theme(legend.position = \"none\")\n",
        "\n",
        "# grid.arrange(plot1, plot2, ncol = 2)\n",
        "\n",
        "\n",
        "# # Test Kruskal-Wallis -> H0: Forma de Distribuciones =\n",
        "# print(kruskal.test(n_huevos ~ condicion, data = datos))\n",
        "\n",
        "# # Kruskal-Wallis Multiple Comparation\n",
        "# # library(pgirmess)\n",
        "# # print(kruskalmc(datos$n_huevos ~ datos$condicion))\n",
        "\n",
        "# # CHIobs > CHIcrit"
      ],
      "metadata": {
        "id": "jWVBmiC5_5G-"
      },
      "id": "jWVBmiC5_5G-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ANOVA Dos Vias\n",
        "Variable Dependiente Cuantitativa y Dos Predictoras (Factor) Categoricas"
      ],
      "metadata": {
        "id": "oZrnWxI_ibQg"
      },
      "id": "oZrnWxI_ibQg"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generamos el Dataset\n",
        "# %%R\n",
        "\n",
        "# resistencia <- c(15.29, 15.89, 16.02, 16.56, 15.46, 16.91, 16.99,\n",
        "# 17.27, 16.85, 16.35, 17.23, 17.81, 17.74, 18.02, 18.37, 12.07, 12.42,\n",
        "# 12.73, 13.02, 12.05, 12.92, 13.01, 12.21, 13.49, 14.01, 13.30, 12.82,\n",
        "# 12.49, 13.55, 14.53)\n",
        "# templado <- c(rep(c('rapido', 'lento'), c(15,15)))\n",
        "# grosor <- rep(c(8, 16, 24), each = 5, times = 2)\n",
        "# datos <- data.frame(templado = templado, grosor = as.factor(grosor), resistencia = resistencia)\n",
        "# print(head(datos))"
      ],
      "metadata": {
        "id": "-aOAYgeVlxka"
      },
      "id": "-aOAYgeVlxka",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### “Boxplot” para identificar posibles diferencias significativas, asimetrias, valores atipicos y homogeneidad de varianza entre los distintos niveles.\n"
      ],
      "metadata": {
        "id": "-rij2heybVVl"
      },
      "id": "-rij2heybVVl"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# # library(gridExtra)\n",
        "# # library(ggplot2)\n",
        "\n",
        "# # Analizamos BoxPlots\n",
        "# p1 <- ggplot(data = datos, mapping = aes(x = templado, y = resistencia)) + geom_boxplot() + theme_bw()\n",
        "# p2 <- ggplot(data = datos, mapping = aes(x = grosor, y = resistencia)) + geom_boxplot() + theme_bw()\n",
        "# p3 <- ggplot(data = datos, mapping = aes(x = templado, y = resistencia, colour = grosor)) + geom_boxplot() + theme_bw()\n",
        "\n",
        "# grid.arrange(p1, p2, ncol = 2)\n",
        "# plot(p3)"
      ],
      "metadata": {
        "id": "B60dBZrZXiL1"
      },
      "id": "B60dBZrZXiL1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Graficos de Interaccion: Paralelos es porque no hay interaccion"
      ],
      "metadata": {
        "id": "5rSUjTLAbbkH"
      },
      "id": "5rSUjTLAbbkH"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# # Interaccion Templado con Grosor\n",
        "# plot1 <- ggplot(data = datos, aes(x = templado, y = resistencia, colour = grosor,\n",
        "# group = grosor)) +\n",
        "# stat_summary(fun = mean, geom = 'point') +\n",
        "# stat_summary(fun = mean, geom = 'line') +\n",
        "# labs(y = 'mean (resistencia)') + theme_bw()\n",
        "\n",
        "# # Interaccion Grsor con Templado\n",
        "# plot2 <- ggplot(data = datos, aes(x = grosor, y = resistencia, colour = templado,\n",
        "# group = templado)) +\n",
        "# stat_summary(fun = mean, geom = 'point') +\n",
        "# stat_summary(fun = mean, geom = 'line') +\n",
        "# labs(y = 'mean (resistencia)') + theme_bw()\n",
        "\n",
        "# grid.arrange(plot1, plot2, ncol = 2)"
      ],
      "metadata": {
        "id": "xBEfuaoQbhtD"
      },
      "id": "xBEfuaoQbhtD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Estimacion de Coeficientes"
      ],
      "metadata": {
        "id": "mjILCvXCXybG"
      },
      "id": "mjILCvXCXybG"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# # library(lsr)\n",
        "# # library(lmtest)\n",
        "\n",
        "# model <- aov(resistencia ~ templado * grosor, data=datos)\n",
        "# print(summary(model))\n",
        "\n",
        "# # Eta Cuadrado: Proporcion de Varianza explicada por el efecto mas el error\n",
        "# print(etaSquared(model))"
      ],
      "metadata": {
        "id": "UEgCiDXpbfuk"
      },
      "id": "UEgCiDXpbfuk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #“Boxplot” para identificar posibles diferencias significativas, asimetrias, valores atipicos y homogeneidad de varianza entre los distintos niveles.\n",
        "# %%R\n",
        "\n",
        "# library(ggplot2)\n",
        "\n",
        "# # Plot\n",
        "# p1 <- ggplot(data = datos, aes(x = templado, y = resistencia,fill=templado)) + geom_boxplot() +\n",
        "# theme_bw()+scale_fill_brewer(palette='Dark2')\n",
        "# p2 <- ggplot(data = datos, aes(x = grosor, y = resistencia, fill =grosor))+ geom_boxplot() + theme_bw()+scale_fill_brewer(palette='Dark2')\n",
        "# p3 <- ggplot(data = datos, aes(x = templado, y = resistencia, colour = grosor)) + geom_boxplot() + theme_bw()+scale_fill_brewer(palette='Dark2')\n",
        "\n",
        "# plot(p1)\n",
        "# plot(p2)\n",
        "# plot(p3)"
      ],
      "metadata": {
        "id": "2q9OzKDNkBE9"
      },
      "id": "2q9OzKDNkBE9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Analisis de Diagnostico\n",
        "# %%R\n",
        "\n",
        "# print(shapiro.test(model$resid))\n",
        "# print(bptest(model))\n",
        "# print(dwtest(model, alternative='two.sided', iterations=1000))\n",
        "\n",
        "# plot(model) # verificamos Outliers e Influyentes para probar sacarlos mejora el modelo\n",
        "\n",
        "# # Los residuos no tienen problemas, y la interaccion no es significativa. Se elimina del modelo"
      ],
      "metadata": {
        "id": "rSaQqsMThuci"
      },
      "id": "rSaQqsMThuci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresion Logistica\n",
        "Modela el Logaritmo de Cocientes de Probabilidades Ln(p/q) = Logit"
      ],
      "metadata": {
        "id": "y-CbmfN7YUyW"
      },
      "id": "y-CbmfN7YUyW"
    },
    {
      "cell_type": "code",
      "source": [
        "# # EDA\n",
        "\n",
        "# data = dataSalmon\n",
        "\n",
        "# # Transformo Target en 0 y 1\n",
        "# data['Origen'] = [0 if val == 'Alaska' else 1 for val in data.origen]\n",
        "# data.drop(columns='origen', inplace=True)\n",
        "\n",
        "# # Exporto a R\n",
        "# %R -i data\n",
        "\n",
        "# # Grafico de Dispersion\n",
        "# # sns.scatterplot(data=data, x='aguadulce', y='mar', hue='Origen')\n",
        "# # plt.show()\n",
        "\n",
        "\n",
        "# # Distribucion de cada Variable Predictora\n",
        "# sns.displot(data=data, x='aguadulce', hue='Origen', kind='kde')\n",
        "# plt.show()\n",
        "\n",
        "# sns.displot(data=data, x='mar', hue='Origen', kind='kde')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "q4BeXUCEYxXX"
      },
      "id": "q4BeXUCEYxXX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# # Armamos el modelo\n",
        "# model_logi <- glm(Origen ~ aguadulce, data=data, family = 'binomial')\n",
        "# print(summary(model_logi))\n",
        "\n",
        "# # Ploteamos la Logistica\n",
        "# plot(x = data$aguadulce, y = data$Origen, col = \"darkblue\", main = \"probabilidad según origen\", xlab = \"Cantidad en agua dulce\", ylab = \"Probabilidad según origen\")\n",
        "# curve(predict(model_logi, data.frame(aguadulce=x), type = \"response\"), add = TRUE, col = \"firebrick\", lwd = 2.5)\n",
        "\n",
        "# # Aramamos el IC\n",
        "# print(confint(object=model_logi, level=0.95))"
      ],
      "metadata": {
        "id": "v1DbZI41Y6l3"
      },
      "id": "v1DbZI41Y6l3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Predecimos\n",
        "# %%R\n",
        "# library(ggplot2)\n",
        "\n",
        "# # Valores random\n",
        "# new_data <- seq(from = min(data$aguadulce), to = max(data$aguadulce), by = 0.5)\n",
        "\n",
        "# # Predecimos\n",
        "# predicciones <- predict(model_logi, data.frame(aguadulce=new_data), se.fit=TRUE)\n",
        "\n",
        "# # Transformamos en probabilidades\n",
        "# predicciones_logit <- exp(predicciones$fit)/(1+exp(predicciones$fit))\n",
        "\n",
        "# # Limites del IC al 95%\n",
        "# limite_inferior <- predicciones$fit - 1.96 * predicciones$se.fit\n",
        "# limite_inferior_logit <- exp(limite_inferior)/(1 + exp(limite_inferior))\n",
        "# limite_superior <- predicciones$fit + 1.96 * predicciones$se.fit\n",
        "# limite_superior_logit <- exp(limite_superior)/(1 + exp(limite_superior))\n",
        "\n",
        "# # Ploteamos\n",
        "# datos_curva <- data.frame(aguadulce = new_data, probabilidad_aguadulce = predicciones_logit, limite_inferior_logit = limite_inferior_logit, limite_superior_logit = limite_superior_logit)\n",
        "# plot1 <- ggplot(data, aes(x = aguadulce, y = Origen)) +\n",
        "#   geom_point(aes(color = as.factor(Origen)),shape = \"I\", size = 3) +\n",
        "#   geom_line(data = datos_curva, aes(y = probabilidad_aguadulce), color = \"firebrick\") +\n",
        "#   geom_line(data = datos_curva, aes(y = limite_inferior_logit), linetype = \"dashed\") +\n",
        "#   geom_line(data = datos_curva, aes(y = limite_superior_logit), linetype = \"dashed\") +\n",
        "#   theme_bw() +\n",
        "#   labs(title = \"Modelo regresión logística Origen ~ aguadulce\", y = \"P(Origen = Canadá | aguadulce)\", y = \"Origen\") +\n",
        "#   theme(legend.position = \"null\") + theme(plot.title = element_text(hjust = 0.5))\n",
        "# # plot(plot1)\n",
        "\n",
        "# # Deviance (diferencia de Residuos) -> p-value < .05 El modelo es Significativo\n",
        "# print(anova(model_logi, test='Chisq'))"
      ],
      "metadata": {
        "id": "E1Ul9UusY9bG"
      },
      "id": "E1Ul9UusY9bG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Agregamos mas variables al modelo\n",
        "# %%R\n",
        "\n",
        "# # library(lmtest)\n",
        "\n",
        "# model_logi2 <- glm(Origen ~ aguadulce + mar, data=data, family='binomial')\n",
        "# # print(summary(model_logi2))\n",
        "\n",
        "# # Pruebas de verosimilitud -> H0: Modelos similares, H1: Modelo mejor\n",
        "# print(lrtest(model_logi, model_logi2))"
      ],
      "metadata": {
        "id": "vZskLrlNZEoS"
      },
      "id": "vZskLrlNZEoS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test de Hosmer-Lemeshow"
      ],
      "metadata": {
        "id": "0HeYmnOmZb4W"
      },
      "id": "0HeYmnOmZb4W"
    },
    {
      "cell_type": "code",
      "source": [
        "# # H0 -> Las Proporciones Observadas de eventos son similares a las\n",
        "# # Proporciones Predichas en cada subgrupo (El modelo se ajusta a los datos)\n",
        "# %%R\n",
        "\n",
        "# library(ResourceSelection)\n",
        "# HL <- hoslem.test(data$Origen, model_logi2$fitted.value)\n",
        "# print(HL)\n",
        "# print(cbind(HL$observed, HL$expected))"
      ],
      "metadata": {
        "id": "oSxYv_p1ZHvo"
      },
      "id": "oSxYv_p1ZHvo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Curvas ROC y Metricas"
      ],
      "metadata": {
        "id": "80q1QsYochr5"
      },
      "id": "80q1QsYochr5"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generamos la Matriz de Confusion\n",
        "# %%R\n",
        "\n",
        "# # library(vcd)\n",
        "\n",
        "# predicciones <- ifelse(test = model_logi$fitted.values > 0.5, yes = 1, no = 0)\n",
        "# matriz_confusion <- table(data$Origen, predicciones, dnn = c(\"observaciones\", \"predicciones\"))\n",
        "# print(matriz_confusion)\n",
        "\n",
        "\n",
        "# # Ploteamos\n",
        "# mosaic(matriz_confusion, shade = T, colorize = T, gp = gpar(fill = matrix(c(\"green3\", \"red2\", \"red2\", \"green3\"), 2, 2)))"
      ],
      "metadata": {
        "id": "UdKWEU-tclZH"
      },
      "id": "UdKWEU-tclZH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Separamos Train y Test\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X = data.drop(columns='Origen')\n",
        "# y = data['Origen']\n",
        "\n",
        "# Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.4, stratify=y)\n",
        "\n",
        "# train = pd.concat([Xtrain, ytrain], axis=1)\n",
        "# test = pd.concat([Xtest, ytest], axis=1)\n",
        "\n",
        "# %R -i train\n",
        "# %R -i test\n",
        "# %R -i Xtest\n",
        "# %R -i ytest\n",
        "# # %R -i data"
      ],
      "metadata": {
        "id": "E6PyTn3he-z0"
      },
      "id": "E6PyTn3he-z0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# # library(pROC)\n",
        "\n",
        "# # Modelamos separando en Train y Test\n",
        "# model_logi3 <- glm(Origen ~ aguadulce + mar, data=train, family='binomial')\n",
        "# predicciones <- predict(object = model_logi3, newdata = Xtest, type = \"response\")\n",
        "\n",
        "# # Ploteamos la curva ROC\n",
        "# curva_roc <- roc(response = ytest, predictor = predicciones)\n",
        "# plot(curva_roc,col=\"red\",lwd=2,main=\"ROC test\")\n",
        "# legend(\"bottomright\",legend=paste(\"AUC=\",round(auc(curva_roc),4)))"
      ],
      "metadata": {
        "id": "B8AnUJ-Hfe-V"
      },
      "id": "B8AnUJ-Hfe-V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metricas de Interes"
      ],
      "metadata": {
        "id": "eevkFSySiO8j"
      },
      "id": "eevkFSySiO8j"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Precision : TP / (TP+FP)\n",
        "# # Recall : TP / (TP+FN)\n",
        "# %%R\n",
        "\n",
        "# umbral = 0.5\n",
        "\n",
        "# actual_values<-ytest\n",
        "# predict_value<-predicciones\n",
        "# cm <- table(ACTUAL=actual_values,PREDICTED=predict_value>umbral) # asumimos umbral de 0.5\n",
        "# print(cm)\n",
        "\n",
        "# pred<-predict_value>umbral\n",
        "# TP<-length(actual_values[(actual_values==1)&(pred==1)])#15\n",
        "# TN<-length(actual_values[(actual_values==0)&(pred==0)])#12\n",
        "# FP<-length(actual_values[(actual_values==0)&(pred==1)])#1\n",
        "# FN<-length(actual_values[(actual_values==1)&(pred==0)])#2\n",
        "# precision<-TP/(TP+FP)\n",
        "# recall<-TP/(TP+FN)\n",
        "# accuracy <- (TP+TN)/(TP+TN+FN+FP)\n",
        "# f1_score<-(2*precision*recall)/(precision+recall)\n",
        "\n",
        "# cat('Precision:', precision, 'Recall:', recall, 'Accuracy:', accuracy, 'F1-Score:', f1_score)"
      ],
      "metadata": {
        "id": "IxVqKYbdiSAY"
      },
      "id": "IxVqKYbdiSAY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z402C7ZFlyeN"
      },
      "id": "z402C7ZFlyeN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}