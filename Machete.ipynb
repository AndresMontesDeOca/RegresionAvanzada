{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndresMontesDeOca/RegresionAvanzada/blob/main/Machete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eda0864a",
      "metadata": {
        "id": "eda0864a"
      },
      "source": [
        "# MACHETE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logbook"
      ],
      "metadata": {
        "id": "ZrfIpM4oo-bq"
      },
      "id": "ZrfIpM4oo-bq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Autor: Andres Montes de Oca\n",
        "\n",
        "# 31/05/23 -> Creacion de la Notebook\n",
        "# 31/05/23 -> Tests Normalidad, Homocedasticidad Residuos\n",
        "# 06/06/23 -> Test No-autoorrelacion de Residuos\n",
        "# 17/06/23 -> Transfomraciones Box Clox\n",
        "# 19/06/23 -> Migrated to Google Colab\n",
        "# 20/06/23 -> R Magic\n",
        "# 22/06/23 -> Deteccion Outliers e Influyentes"
      ],
      "metadata": {
        "id": "HO1BXrWJo83M"
      },
      "id": "HO1BXrWJo83M",
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf8c17a",
      "metadata": {
        "id": "bbf8c17a"
      },
      "outputs": [],
      "source": [
        "# Instalacion de Paquetes\n",
        "!pip install pingouin # No incluido en Google Colab\n",
        "# !pip install scipy\n",
        "# !pip install statsmodels\n",
        "\n",
        "# Cargamos Librerias y Datos\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "from scipy import stats as st\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.tools.tools as smt\n",
        "\n",
        "# Ignorar Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Asthetics\n",
        "sns.set(style='ticks', context='notebook', palette='colorblind', font_scale=1, color_codes=True)\n",
        "\n",
        "# Recursion limit errors with R Magic\n",
        "import sys\n",
        "# sys.setrecursionlimit(50000)\n",
        "\n",
        "# Activamos R magic\n",
        "%load_ext rpy2.ipython"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf6ec746",
      "metadata": {
        "id": "bf6ec746"
      },
      "source": [
        "### Libreria rpy2 (Python <==> R)\n",
        "- No la usamos ya que en Colab disponemos de R Magic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "id": "436c2819",
      "metadata": {
        "id": "436c2819"
      },
      "outputs": [],
      "source": [
        "# Version rpy2 que no tiene problemas de compatibilidad\n",
        "# !pip install rpy2==3.5.1\n",
        "\n",
        "# Import rpy2 for dataframe conversion\n",
        "# import rpy2.robjects as ro\n",
        "# from rpy2.robjects.packages import importr\n",
        "# from rpy2.robjects import pandas2ri\n",
        "# from rpy2.robjects.conversion import localconverter\n",
        "# from rpy2.robjects import globalenv\n",
        "\n",
        "\n",
        "# ###### Pandas DataFrames and Series conversion ########\n",
        "# # Cargamos un DataSet cualquira en Python, para que no de error\n",
        "# data_P = sns.load_dataset('iris')\n",
        "# Serie = data_P['petal_length']\n",
        "\n",
        "# # Convert the Python DataFrame to the R dataframe\n",
        "# %R -i data_P\n",
        "\n",
        "# with localconverter(ro.default_converter + pandas2ri.converter):\n",
        "#   data_R = ro.conversion.py2rpy(data_P)\n",
        "# # Create a variable name in R's Global Environment\n",
        "# globalenv['data_R'] = data_R\n",
        "\n",
        "# # Convert Python Series to R vectors\n",
        "# vec_float_R = ro.vectors.FloatVector(Serie)\n",
        "# # vec_int_R = ro.vectors.IntVector(Serie)\n",
        "# # vec_str_R = ro.vectors.StrVector(Serie)\n",
        "# globalenv['vec_float_R'] = vec_float_R\n",
        "\n",
        "# # Convert R datadrame/vector to Python DataFrame/Vector\n",
        "# %R -o data_R\n",
        "\n",
        "# with localconverter(ro.default_converter + pandas2ri.converter):\n",
        "#   data_P = ro.conversion.rpy2py(data_R)\n",
        "\n",
        "# # Importar R-Functions a Python como Objetos(sin uso)\n",
        "# shapiro_test = ro.r('shapiro.test')\n",
        "# result = shapiro_test(vec_float_R)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instalaciones Individuales en Bash"
      ],
      "metadata": {
        "id": "KfM50m-wCz4b"
      },
      "id": "KfM50m-wCz4b"
    },
    {
      "cell_type": "code",
      "source": [
        "# system(sudo apt install libgsl-dev) # -> Rapido"
      ],
      "metadata": {
        "id": "pZI0hXx4DbdA"
      },
      "execution_count": 354,
      "outputs": [],
      "id": "pZI0hXx4DbdA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instalacion de Librerias para R en Google Colab"
      ],
      "metadata": {
        "id": "RWsE_ORpdjnL"
      },
      "id": "RWsE_ORpdjnL"
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "\n",
        "### MVN Henze-Zirkler Test ### -> Lento\n",
        "# # system(sudo apt install libgsl-dev)\n",
        "# install.packages('MVN')\n",
        "\n",
        "# ## Wald Test ### -> Rapido\n",
        "# install.packages('aod')\n",
        "\n",
        "# ## QQ Plot, Bonferroni ### -> Lento\n",
        "# install.packages('car')\n",
        "\n",
        "# # LRM Tests ### -> Rapido\n",
        "# install.packages('lmtest')\n",
        "\n",
        "# ## Box-Cox Transformation ### -> Rapido\n",
        "# install.packages('MASS')\n",
        "\n",
        "\n",
        "####################################\n",
        "### Installation Status ###\n",
        "# print(system.file(package='MVN'))"
      ],
      "metadata": {
        "id": "8ufTcCWTEAvQ",
        "outputId": "5d4d6c79-9770-4d5f-80eb-df442d42cbbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8ufTcCWTEAvQ",
      "execution_count": 355,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NULL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b00fc712",
      "metadata": {
        "id": "b00fc712"
      },
      "source": [
        "## Cargas de Datasets from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "id": "74cd8b72",
      "metadata": {
        "id": "74cd8b72"
      },
      "outputs": [],
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "################################# Datasets ###################################\n",
        "# Grasa de Cerdos\n",
        "id = '153lGVzdixcHT-keKg8qmvaoWdPHg6_tB'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('grasacerdos.xlsx')\n",
        "dataG = pd.read_excel('grasacerdos.xlsx', index_col='Obs') # Cargamos el Dataset\n",
        "dataG = dataG.replace(to_replace=',', value='.', regex=True) # Reemplazo , por .\n",
        "dataG = dataG.astype('float') # Transformo en float\n",
        "%R -i dataG\n",
        "\n",
        "######################################################\n",
        "# Peso, Edad, Colesterol\n",
        "id = '17Dv1WcWlc9ojWa6bnSfD7TCRaVKCKqj0'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('peso_edad_colest.xlsx')\n",
        "dataP = pd.read_excel('peso_edad_colest.xlsx')\n",
        "%R -i dataP\n",
        "\n",
        "# # Convert the Python DataFrame to the R dataframe\n",
        "# with localconverter(ro.default_converter + pandas2ri.converter):\n",
        "#   dataP_R = ro.conversion.py2rpy(dataP)\n",
        "# # Create a variable name in R's Global Environment\n",
        "# globalenv['dataP_R'] = dataP_R\n",
        "\n",
        "######################################################\n",
        "# Cars con Modelo Lineal\n",
        "id = '15pf-6P4Ek2rp6mYmfOybHRqAxqOoTp6i'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('cars.csv')\n",
        "dataC = pd.read_csv('cars.csv')\n",
        "model_cars = smf.ols('dist ~ speed', data=dataC).fit()\n",
        "%R -i dataC\n",
        "\n",
        "######################################################\n",
        "# University\n",
        "id = '16kLQNXhyweAi38xR2IeQCCagVcU725UZ'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('University.csv')\n",
        "dataU = pd.read_csv('University.csv')\n",
        "%R -i dataU\n",
        "\n",
        "######################################################\n",
        "# Iris(Python)\n",
        "dataI = sns.load_dataset('iris')\n",
        "\n",
        "# # Iris(R) -> DataFrame to dataframe\n",
        "# with localconverter(ro.default_converter + pandas2ri.converter):\n",
        "#   dataI_R = ro.conversion.py2rpy(dataI.drop(columns='species'))\n",
        "# # Create a variable name in R's global environment\n",
        "# globalenv['dataI_R'] = dataI_R\n",
        "\n",
        "# # Serie to Vector\n",
        "# x_R = ro.FloatVector(dataI['petal_length'])\n",
        "# w_R = ro.FloatVector(dataI['petal_width'])\n",
        "# globalenv['x_R'] = x_R\n",
        "# globalenv['w_R'] = w_R\n",
        "%R -i dataI\n",
        "\n",
        "######################################################\n",
        "# Gorriones\n",
        "id = '15G6jIOMiuWaTs-qy7eUXwhED4delBMl6'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('gorriones.xlsx')\n",
        "dataO = pd.read_excel('gorriones.xlsx')\n",
        "dataO.rename(columns=\n",
        "            {'largototal':'Largo', 'extension ':'Alas', 'cabeza':'Cabeza',\n",
        "             'humero':'Pata', 'esternon':'Cuerpo', 'sobrevida ':'Target'},\n",
        "            inplace=True)\n",
        "%R -i dataO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis de Variables"
      ],
      "metadata": {
        "id": "Bj2eFW6ZFMUD"
      },
      "id": "Bj2eFW6ZFMUD"
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "id": "cf05b562",
      "metadata": {
        "id": "cf05b562"
      },
      "outputs": [],
      "source": [
        "# data=dataI\n",
        "\n",
        "# # Relacion Lineal Grafica\n",
        "# sns.scatterplot(data=data, x='petal_length', y='petal_width')\n",
        "# plt.show()\n",
        "\n",
        "# # Multicollonearity - Eeach independent variable should be independent from other independent variables\n",
        "# print(pg.corr(data['petal_length'], data['petal_width'], method='pearson')) # Entre dos varibales, H0 -> Variables Independientes\n",
        "# print(pg.corr(data['petal_length'], data['petal_width'], method='spearman')) # Cuando no se cumplen los supuestos de Pearson\n",
        "# display(sns.heatmap(data.corr(), vmin=-1, vmax=1, cmap='RdYlGn', annot=True)) # Todos contra todos\n",
        "\n",
        "# # Analsis de Noramlidad Multi-Variariable (Henze-Zirkler) -> Python\n",
        "# print(pg.multivariate_normality(data.drop(columns='species'))) # Henze-Zirkler Test, H0 -> Multiv Normal Dist\n",
        "\n",
        "# # Analsis de Noramlidad Multi-Variariable (Henze-Zirkler) -> R\n",
        "# # library(MVN)\n",
        "# # mvn_result <- mvn(data, mvnTest = 'hz')\n",
        "# # print(mvn_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Models"
      ],
      "metadata": {
        "id": "aqpTI97ucr0F"
      },
      "id": "aqpTI97ucr0F"
    },
    {
      "cell_type": "code",
      "source": [
        "# # En R\n",
        "\n",
        "# %%R\n",
        "# data <- dataP\n",
        "\n",
        "# model <- lm('colest ~ edad', data=data)\n",
        "# resid = model$resid\n",
        "# predicted = predict(model)\n",
        "# fitted = model$fitted.values\n",
        "# # print(summary(model_colest_R))\n",
        "# # print(anova(model_colest_R))\n",
        "\n",
        "# # Bandas de Prediccion\n",
        "# predichos <- predict(object=model, interval='prediction', level=0.95)\n",
        "# nuevos_datos <- data.frame(predichos, data)\n",
        "\n",
        "# head(nuevos_datos)"
      ],
      "metadata": {
        "id": "H4kJn7W3Fas6"
      },
      "id": "H4kJn7W3Fas6",
      "execution_count": 386,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Graficamos en Python\n",
        "# %R -o nuevos_datos\n",
        "\n",
        "# # Grafico\n",
        "# sns.regplot(data=nuevos_datos, x='edad', y='colest')\n",
        "# sns.lineplot(data=nuevos_datos, x='edad', y='lwr', linestyle='--', color='r')\n",
        "# sns.lineplot(data=nuevos_datos, x='edad', y='upr', linestyle='--', color='r')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "0eh_UlkkFhXL"
      },
      "id": "0eh_UlkkFhXL",
      "execution_count": 387,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # LRM en Python\n",
        "# data = dataP\n",
        "\n",
        "# # Generamos el Modelo\n",
        "# data = sm.add_constant(data)\n",
        "# model = smf.ols('colest ~ edad', data=data).fit()\n",
        "\n",
        "# # Ploteamos\n",
        "# sns.regplot(data=data, x='edad', y='colest')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "BRe7V4KrcsGT"
      },
      "id": "BRe7V4KrcsGT",
      "execution_count": 388,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wald Test (revisar)\n",
        "Mide si la Variable X es lo suficientemente significativa como para explicar la"
      ],
      "metadata": {
        "id": "Vn93BkqUKkOb"
      },
      "id": "Vn93BkqUKkOb"
    },
    {
      "cell_type": "code",
      "source": [
        "# #####   PENDIENTE    ######\n",
        "# %%R\n",
        "# data <- dataO\n",
        "\n",
        "# library(aod)\n",
        "\n",
        "# # Generamos el Modelo\n",
        "# model_gorr = lm('Largo ~ Alas + Cabeza + Pata + Cuerpo', data=data)\n",
        "# print(summary(model_gorr))\n",
        "\n",
        "# # Wald Test para indentificar variables significativas\n",
        "# wald.test(Sigma = vcov(model_gorr), b = coef(model_gorr), Terms = 4) # Cuerpo| H0: Coef = 0 (Var no significativa)\n"
      ],
      "metadata": {
        "id": "u2u1biCGKpGn"
      },
      "id": "u2u1biCGKpGn",
      "execution_count": 399,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis de Diagnostico"
      ],
      "metadata": {
        "id": "UF-d2Q40JuGM"
      },
      "id": "UF-d2Q40JuGM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residuals Nomrality"
      ],
      "metadata": {
        "id": "6PS87hllKYD-"
      },
      "id": "6PS87hllKYD-"
    },
    {
      "cell_type": "code",
      "source": [
        "# %%R\n",
        "\n",
        "# data <- dataI\n",
        "\n",
        "# # Analisis de Normalidad en los Residuos\n",
        "# library(car)\n",
        "# qqPlot(resid) # Observaciones que romperian con el supuesto de Normalidad\n",
        "# print(shapiro.test(resid))"
      ],
      "metadata": {
        "id": "eSLP7Yw4KWnH"
      },
      "id": "eSLP7Yw4KWnH",
      "execution_count": 401,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Python\n",
        "# data = dataI\n",
        "\n",
        "# # Tests Analiticos\n",
        "# display(pg.normality(data['sepal_width'])) # Shapiro Test(single Var), H0 -> Normal Dist\n",
        "# print(st.anderson(dataI['sepal_width'], dist='norm')) # Anderson-Darling SciPy\n",
        "# print(st.kstest(dataI['sepal_width'], 'norm'))# Kolmogorov-Smirnov SciPy\n",
        "\n",
        "# # Tests Graficos\n",
        "# pg.qqplot(data['sepal_width'])\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "GHx3kYbQH16s"
      },
      "id": "GHx3kYbQH16s",
      "execution_count": 403,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6a56b196",
      "metadata": {
        "id": "6a56b196"
      },
      "source": [
        "### Residuals Variance (Homosedasticity)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Analisis Analitico (Breusch-Pagan Test) -> R | H0: Residuos Homocedasticos\n",
        "# %%R\n",
        "\n",
        "# library(lmtest)\n",
        "# bptest(model)"
      ],
      "metadata": {
        "id": "aM8aPx10LXlm"
      },
      "id": "aM8aPx10LXlm",
      "execution_count": 405,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 407,
      "id": "94129b70",
      "metadata": {
        "id": "94129b70"
      },
      "outputs": [],
      "source": [
        "# # # Analsis Grafico -> Python\n",
        "\n",
        "# # Importo Datos desde R\n",
        "# %R -o resid\n",
        "# %R -o predicted\n",
        "# %R -o fitted\n",
        "\n",
        "# plt.scatter(x=predicted, y=resid)\n",
        "# plt.xlabel( 'Prediccion')\n",
        "# plt.ylabel('Residuo')\n",
        "# plt.title('Distribucion de Residuos')\n",
        "# plt.axhline(color='grey', linestyle='dashed', alpha=0.5)\n",
        "# plt.show()\n",
        "# # No se observa estructura de embudo\n",
        "\n",
        "# # # Analisis Analitico Python\n",
        "# BP, p_value, _, _ =sm.stats.het_breuschpagan(model.resid, model.model.exog)\n",
        "# print('Estadistico BP y p-value:', BP, p_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5bc19b",
      "metadata": {
        "id": "cd5bc19b"
      },
      "source": [
        "### Residuals Auto-Correlation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Analisis Analitico (Durbin-Watson Test) -> R | H0 No Auto-Correlacion\n",
        "# # 2=No Correlacion (Independecia)| 0=Correlacion Pos | 4=Correlacion Neg\n",
        "# %%R\n",
        "\n",
        "# library(lmtest)\n",
        "# dwtest(model, alternative='two.sided', iterations=1000)"
      ],
      "metadata": {
        "id": "aLbQWQRJMp2S"
      },
      "id": "aLbQWQRJMp2S",
      "execution_count": 412,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 413,
      "id": "fb3fbc04",
      "metadata": {
        "id": "fb3fbc04"
      },
      "outputs": [],
      "source": [
        "# # Analsis Grafico -> Python\n",
        "# data=dataP\n",
        "# plt.scatter(x=data.index, y=resid)\n",
        "# plt.xlabel( 'Index')\n",
        "# plt.ylabel('Residuo')\n",
        "# plt.title('Correlacion de Residuos')\n",
        "# plt.axhline(color='grey', linestyle='dashed', alpha=0.5)\n",
        "# plt.show()\n",
        "# # No se observa estructura\n",
        "\n",
        "# # Analisis Analitico -> Python\n",
        "# print('Durbin-Watson:', sm.stats.durbin_watson(resid)) # Sin validacion Estadistica"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformaciones\n",
        "- Cuando los residuos no siguen una distribucion Normal -> Transformamos la y (BoxCox)\n",
        "- Cuando los residuos tienen estructura -> Transformamos/Agregamos X"
      ],
      "metadata": {
        "id": "vevQU54sOiTx"
      },
      "id": "vevQU54sOiTx"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Box Cox Transformations -> R\n",
        "# %%R\n",
        "# data = dataC\n",
        "# library(MASS)\n",
        "\n",
        "# # Generamos el modelo y verificamos la Normalidad de los residuos\n",
        "# model <- lm('dist ~ speed', data=data)\n",
        "# print(shapiro.test(model$resid))\n",
        "\n",
        "# # Buscamos el Lambda  optimo -> Si Lambda  = 0 -> log10(y) | y**Lambda\n",
        "# boxcox(object = model, plotit=TRUE)"
      ],
      "metadata": {
        "id": "yy2EvWj_OuWR"
      },
      "id": "yy2EvWj_OuWR",
      "execution_count": 427,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Transformamos/Agregamos Variables en Dataset original -> Python\n",
        "# data = dataC\n",
        "\n",
        "# dist_p2 = pd.Series(data['dist']**2, name='dist_p2')\n",
        "# dist_log10 = pd.Series(np.log10(data['dist']), name='dist_log10')\n",
        "\n",
        "# dataC2 = pd.concat([data, dist_p2, dist_log10], axis=1)\n",
        "# display(dataC_2.head())\n",
        "\n",
        "# %R -i dataC2 # Importamos a R"
      ],
      "metadata": {
        "id": "PRIyPS3rOubS"
      },
      "id": "PRIyPS3rOubS",
      "execution_count": 437,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EniADp3MZZtc"
      },
      "id": "EniADp3MZZtc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Generamos el nuevo modelo con las nuevas variables transformadas, y veridicamos su Normalidad\n",
        "# %%R\n",
        "\n",
        "# model_2 <- lm('dist_p2 ~ speed', data=dataC2)\n",
        "# print(shapiro.test(model_2$resid))\n",
        "\n",
        "# # Bandas de Prediccion\n",
        "# predichos <- predict(object=model_2, interval='prediction', level=0.95)\n",
        "# nuevos_datos <- data.frame(predichos, dataC2)\n",
        "\n",
        "# head(nuevos_datos)"
      ],
      "metadata": {
        "id": "ONnQf1q_SnFj"
      },
      "id": "ONnQf1q_SnFj",
      "execution_count": 436,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Graficamos con las Bandas de Confianza y Prediccion\n",
        "# %R -o nuevos_datos # Exportamos desde R\n",
        "\n",
        "# sns.regplot(data=nuevos_datos, x='speed', y='dist_p2')\n",
        "# sns.lineplot(data=nuevos_datos, x='speed', y='lwr', linestyle='--', color='r')\n",
        "# sns.lineplot(data=nuevos_datos, x='speed', y='upr', linestyle='--', color='r')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "_DyOXIfaZlxs"
      },
      "id": "_DyOXIfaZlxs",
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Deteccion de Valores Outliers e Influyentes"
      ],
      "metadata": {
        "id": "_oOsrltML6nr"
      },
      "id": "_oOsrltML6nr"
    },
    {
      "cell_type": "code",
      "source": [
        "# # No Outliers tests found in Python\n",
        "%%R\n",
        "\n",
        "# library(car)\n",
        "\n",
        "# # Outliers - Bonferroni\n",
        "# qqPlot(residuos)\n",
        "# outlierTest(model_colest_R)\n",
        "# influenceIndexPlot(model_colest_R, vars='Bonf')\n",
        "\n",
        "# # Influyentes - Leverage\n",
        "# crit_leverage <- 3*mean(hatvalues(model_colest_R))\n",
        "# leverage <- hatvalues(model_colest_R) > crit_leverage\n",
        "# print(sum(leverage))\n",
        "# print(cbind(hatvalues(model_colest_R), crit_leverage, leverage))\n",
        "# hist(hatvalues(model_colest_R))\n",
        "\n",
        "# # Influyentes - Cook\n",
        "# print(cooks.distance(model_colest_R))\n",
        "# influenceIndexPlot(model_colest_R, vars='Cook')\n",
        "\n",
        "# # Influyentes - DFFITS (revisar indices en plot)\n",
        "# p <- length(model_colest_R$coefficients)\n",
        "# n <- NROW(model_colest_R$residuals)\n",
        "# dffits_crit <- 2*sqrt(p/n)\n",
        "# dffits <- dffits(model_colest_R)\n",
        "# data_dffits <- data.frame(dffits=dffits)\n",
        "\n",
        "\n",
        "# # Influyentes - DFBetas\n",
        "# dfbetas_crit <- 1 # Umbral estandard\n",
        "# dfbetas(model_colest_R)[,2]>dfbetas_crit\n",
        "\n",
        "# # Resumen\n",
        "# plot(model_colest_R)\n",
        "# summary(influence.measures(model=model_colest_R))\n",
        "# influencePlot(model = model_colest_R)\n"
      ],
      "metadata": {
        "id": "j7O5WyaKL94o",
        "outputId": "7ddd22a6-033d-4b84-961b-ead43ec6b560",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "j7O5WyaKL94o",
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NULL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Exporto a Python y grafico\n",
        "# %R -o data_dffits\n",
        "# %R -o dffits_crit\n",
        "\n",
        "# data_dffits.head()\n",
        "# sns.scatterplot(data=data_dffits, x=data_dffits.index, y='dffits')\n",
        "# plt.axhline(y=dffits_crit, linestyle='--', color='red')\n",
        "# plt.axhline(y=-dffits_crit, linestyle='--', color='red')\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "aCwhB3lvRAll"
      },
      "id": "aCwhB3lvRAll",
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3531f964",
      "metadata": {
        "id": "3531f964"
      },
      "source": [
        "# 9) Transformaciones\n",
        "- Cuando los residuos no siguen una distribucion Normal -> Transformamos la y (BoxCox)\n",
        "- Cuando los residuos tienen estructura -> Transformamos/Agregamos en X"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f84_Lrt2Vxcg"
      },
      "id": "f84_Lrt2Vxcg",
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Las transfomraciones de variable las hago en Python\n",
        "\n",
        "nassets_log10 = pd.Series(np.log10(data['nassets']), name='nassets_log10')\n",
        "stfees_log10 = pd.Series(np.log10(data['stfees']), name='stfees_log10')\n",
        "nassets_logE = pd.Series(np.log(data['nassets']), name='nassets_logE')\n",
        "stfees_logE = pd.Series(np.log(data['stfees']), name='stfees_logE')\n",
        "\n",
        "data = pd.concat([data, nassets_log10, stfees_log10, nassets_logE, stfees_logE], axis=1)\n",
        "%R -i data # Importamos a R"
      ],
      "metadata": {
        "id": "_vm3onAKdGwF",
        "outputId": "19f5807a-4f1c-4f92-ca78-360cd069e4c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "id": "_vm3onAKdGwF",
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'nassets'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-373-7ffb2678e270>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Las transfomraciones de variable las hago en Python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnassets_log10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nassets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nassets_log10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstfees_log10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stfees'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stfees_log10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnassets_logE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nassets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nassets_logE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'nassets'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27dc28e2",
      "metadata": {
        "id": "27dc28e2"
      },
      "outputs": [],
      "source": [
        "# # Boc Cox Transformation Python\n",
        "# # Verificamos la Normalidad de los residuos\n",
        "# print(st.shapiro(model_cars.resid))\n",
        "# data = dataC\n",
        "# y = data['dist']\n",
        "\n",
        "# # Calculo Lambda\n",
        "# maxlog = st.boxcox(y)[1]\n",
        "# st.boxcox_normplot(y, -2, 2, plt)\n",
        "# plt.axvline(maxlog, color='red')\n",
        "# plt.show()\n",
        "\n",
        "# # Transfomracion: Si Lambda  = 0 -> np.log10(y) | y**Lambda\n",
        "# y_trans = pd.Series(y**.5, name='y_trans')\n",
        "# # No olvidarse de anti-transformar para los IC(10**predicted)\n",
        "\n",
        "# # Generamos el nuevo Dataset con la y transformada\n",
        "# data = pd.concat([data, y_trans], axis=1)\n",
        "\n",
        "# # Generamos el nuevo modelo con la y_transformada\n",
        "# model_cars_trans = smf.ols('y_trans ~ speed', data=data).fit()\n",
        "\n",
        "# # Verificamos la nueva normalidad de los residuos\n",
        "# print(pg.normality(model_cars_trans.resid))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10) Cuadrados Minimos Ponderados (WLS)\n",
        "- Cuando los residuos son Heterocedasticos\n",
        "- Se compensa la diferencia en los residuos, asignandole diferentes pesos a las observaciones"
      ],
      "metadata": {
        "id": "cXZiAP2wWaEJ"
      },
      "id": "cXZiAP2wWaEJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cuadrados Minimos Ponderados (WLS) en R\n",
        "# %%R\n",
        "\n",
        "# # Generamos el modelo y verificamos la Heterocedasticidad de los residuos analiticamente\n",
        "# ols_inspec <- lm('inspectores ~ choferes', data=dataCh)\n",
        "# print(bptest(ols_inspec))\n",
        "\n",
        "# # Confirmamos con el analisys grafico en Python\n",
        "\n",
        "# # Generamos la ponderacion de pesos y modelamos de nuevo\n",
        "# pesos = I(1/ols_onspec$fitted.values**2) # A los valores mas alejados se le reduce el peso\n",
        "# pesos2 = 1 / lm(abs(ols_inspec$resid) ~ ols_inspec$fitted.values)$fitted.values**2 # Funciona mejor\n",
        "# pesos3 = 1 / (abs(ols_inspec$resid))#**2\n",
        "\n",
        "# # Modelamos con lo weigths\n",
        "# wls_inspec <- lm('inspectores ~ choferes', data=dataCh, weights=pesos)\n",
        "# print(summary(wls_inspec))\n",
        "\n",
        "# # Verificar como graficar los modelos W en Python"
      ],
      "metadata": {
        "id": "VbT2Q9T9WaNj"
      },
      "id": "VbT2Q9T9WaNj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}